{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../data/VTech/_VTech_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers: 4\n",
      "\tWorker 1: 1 days, 5 trials,  28 transitions,  1,448,215 samples\n",
      "\tWorker 2: 3 days, 4 trials,  68 transitions,  1,351,866 samples\n",
      "\tWorker 3: 1 days, 2 trials,  59 transitions,    859,697 samples\n",
      "\tWorker 4: 1 days, 6 trials, 110 transitions,  1,646,663 samples\n",
      "Total number of transitions: 266.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of workers: {len(df['worker'].unique())}\")\n",
    "for worker, worker_df in df.groupby(\"worker\"):\n",
    "    print(f\"\\tWorker {worker}: {len(worker_df['day'].unique())} days\"\n",
    "          f\", {len(worker_df['trial'].unique())} trials\"\n",
    "          f\", {worker_df['mode'].diff().abs().sum():3.0f} transitions\"\n",
    "          f\", {len(worker_df):10,} samples\")\n",
    "print(f\"Total number of transitions: {df['mode'].diff().abs().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the TF Dataset\n",
    "\n",
    "Stack consecutive `WINDOW_SIZE` samples with a sliding window, use the least sample for the target variable.\n",
    "Do not overlap different workers/trials.\n",
    "The resulting samples in the dataset are: `(WINDOW_SIZE, N_FEATURES) -> (1,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_feature = \"orientation_T8_q0\"\n",
    "WINDOW_SIZE = 50\n",
    "    \n",
    "def make_windowed_dataset(ds, window_size, shift=1):\n",
    "    windows = ds.window(window_size, shift=shift)\n",
    "\n",
    "    def sub_to_batch(sub):\n",
    "        return sub.batch(window_size, drop_remainder=True)\n",
    "\n",
    "    def add_labels(batch):\n",
    "        #return batch[:, :-1], batch[-1, -1]\n",
    "        return batch[:, :-1], batch[0, -1]\n",
    "    \n",
    "    windows = windows.flat_map(sub_to_batch)\n",
    "    return windows.map(add_labels)\n",
    "\n",
    "\n",
    "ds = None\n",
    "for trial_id, trial_data in df.groupby([\"worker\", \"trial\"]):\n",
    "    ds_trial = make_windowed_dataset(\n",
    "        tf.data.Dataset.from_tensor_slices(trial_data.loc[:,first_feature:].values),\n",
    "        window_size=WINDOW_SIZE\n",
    "    )\n",
    "    if ds is None:\n",
    "        ds = ds_trial\n",
    "    else:\n",
    "        ds = ds.concatenate(ds_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 50, 66)            133       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3300)              0         \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 16)                52816     \n",
      "_________________________________________________________________\n",
      "layer2 (Dense)               (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 53,022\n",
      "Trainable params: 52,889\n",
      "Non-trainable params: 133\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "normalization = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "normalization.adapt(ds.take(100000).map(lambda x, y: x))\n",
    "\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(WINDOW_SIZE, 66)),\n",
    "        normalization,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(16, activation=\"relu\", name=\"layer1\"),\n",
    "        keras.layers.Dense(4, activation=\"relu\", name=\"layer2\"),\n",
    "        keras.layers.Dense(1, activation= \"sigmoid\" ),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "41451/41451 [==============================] - 623s 15ms/step - loss: 0.1784 - accuracy: 0.9293\n",
      "Epoch 2/20\n",
      "41451/41451 [==============================] - 600s 14ms/step - loss: 0.2010 - accuracy: 0.9202\n",
      "Epoch 3/20\n",
      "41451/41451 [==============================] - 594s 14ms/step - loss: 0.2109 - accuracy: 0.9151\n",
      "Epoch 4/20\n",
      "41451/41451 [==============================] - 595s 14ms/step - loss: 0.2326 - accuracy: 0.9058\n",
      "Epoch 5/20\n",
      "41451/41451 [==============================] - 592s 14ms/step - loss: 0.2301 - accuracy: 0.9068\n",
      "Epoch 6/20\n",
      "41451/41451 [==============================] - 590s 14ms/step - loss: 0.2455 - accuracy: 0.9012\n",
      "Epoch 7/20\n",
      "41451/41451 [==============================] - 591s 14ms/step - loss: 0.2469 - accuracy: 0.9007\n",
      "Epoch 8/20\n",
      "41451/41451 [==============================] - 591s 14ms/step - loss: 0.2395 - accuracy: 0.9036\n",
      "Epoch 9/20\n",
      "41451/41451 [==============================] - 590s 14ms/step - loss: 0.2384 - accuracy: 0.9034\n",
      "Epoch 10/20\n",
      "41451/41451 [==============================] - 588s 14ms/step - loss: 0.2448 - accuracy: 0.9003\n",
      "Epoch 11/20\n",
      "41451/41451 [==============================] - 592s 14ms/step - loss: 0.2339 - accuracy: 0.9062\n",
      "Epoch 12/20\n",
      "41451/41451 [==============================] - 589s 14ms/step - loss: 0.2329 - accuracy: 0.9052\n",
      "Epoch 13/20\n",
      "41451/41451 [==============================] - 592s 14ms/step - loss: 0.2355 - accuracy: 0.9048\n",
      "Epoch 14/20\n",
      "41451/41451 [==============================] - 587s 14ms/step - loss: 0.2283 - accuracy: 0.9096\n",
      "Epoch 15/20\n",
      "41451/41451 [==============================] - 591s 14ms/step - loss: 0.2257 - accuracy: 0.9100\n",
      "Epoch 16/20\n",
      "41451/41451 [==============================] - 589s 14ms/step - loss: 0.2303 - accuracy: 0.9086\n",
      "Epoch 17/20\n",
      "41451/41451 [==============================] - 580s 14ms/step - loss: 0.2231 - accuracy: 0.9120\n",
      "Epoch 18/20\n",
      "41451/41451 [==============================] - 577s 14ms/step - loss: 0.2217 - accuracy: 0.9137\n",
      "Epoch 19/20\n",
      "41451/41451 [==============================] - 586s 14ms/step - loss: 0.2562 - accuracy: 0.8913\n",
      "Epoch 20/20\n",
      "17806/41451 [===========>..................] - ETA: 5:43 - loss: 0.1934 - accuracy: 0.9246"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.binary_crossentropy, \n",
    "              metrics=['accuracy'])\n",
    "model.fit(ds.shuffle(buffer_size=2**18).batch(128), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([y for _, y in ds])\n",
    "ypred = model.predict(ds.batch(128)).flatten()\n",
    "yhat = np.asarray(ypred > 0.5, dtype=np.float)\n",
    "plt.plot(p[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[:100000])\n",
    "plt.plot(yhat[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y == yhat) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = np.vstack((y, ypred, yhat)).T\n",
    "wavfile.write(\"vtech_preds_late.wav\", 25, wav)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
