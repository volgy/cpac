{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating multiple feature recipes on the CPAC_N10_10_25_20 dataset\n",
    "\n",
    "- **Model**: Gradient Boosted Trees (histogram-based)\n",
    "- **Target(s)**: `TF_Pelvis_Moment_X_BWBH`\n",
    "- **Features**: various (approx. 63 alternative _recipes_)\n",
    "- **Results**: \n",
    "  - $r^2$ scores (by cross-validation)\n",
    "  - feature importances (permutation-based, using the full dataset for training)\n",
    "  - predictions (merged, by cross-validation)\n",
    "- **Evaluation strategy**: cross-validation (leave one subject out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.21\", \"Use the conda_python3_latest kernel!\"\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn import (ensemble, metrics, preprocessing, \n",
    "                     pipeline, inspection, model_selection)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# Local\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"CPAC_N10_10_25_20\"\n",
    "DATASET_CSV = f\"s3://cpac/ORIG/{DATASET}/CPAC10S_N10_10_25_20.csv\"\n",
    "DATASET_README = f\"s3://cpac/ORIG/{DATASET}/READ_ME.xlsx\"\n",
    "RESULTS_DIR = f\"results/{DATASET}\"\n",
    "\n",
    "\n",
    "df_orig = utils.load_dataset(\"s3://cpac/ORIG/CPAC_N10_10_25_20/CPAC10S_N10_10_25_20.csv\")\n",
    "df_orig.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_columns_with_prefix(df, prefix):\n",
    "    columns = []\n",
    "    for column in df.columns:\n",
    "        if column.startswith(prefix):\n",
    "            columns.append(column)\n",
    "    return columns\n",
    "    \n",
    "def get_target_names(df):\n",
    "    return _get_columns_with_prefix(df, \"T_\")\n",
    "\n",
    "def get_meta_names(df):\n",
    "    return _get_columns_with_prefix(df, \"M_\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up dataset\n",
    "\n",
    "- Remove samples based on `M_include_overall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig[df_orig[\"M_include_overall\"] > 0]\n",
    "\n",
    "# Weed out wonky subjects\n",
    "#df = df[df[\"M_Sub\"] in (2,4,5,6,7,8,9)]\n",
    "#RESULTS_DIR += \"_nowonky\"\n",
    "\n",
    "print(f\"Number of samples: {df.shape[0]:,d} (before clean-up: {df_orig.shape[0]:,d})\")\n",
    "print(f\"Number of trials: {len(df['M_Trial_Name'].unique())} (before clean-up: {len(df_orig['M_Trial_Name'].unique())})\")\n",
    "print(f\"Number of subjects: {len(df['M_Sub'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor configurations (recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor_short_name(predictor):\n",
    "    return predictor[17:]\n",
    "\n",
    "def predictor_sensor_number(predictor):\n",
    "    return int(predictor[5:7])\n",
    "\n",
    "def filter_predictors(all_predictors, patterns):\n",
    "    if isinstance(patterns, str):\n",
    "        patterns = (patterns,)\n",
    "        \n",
    "    predictors = []\n",
    "    for predictor in all_predictors:\n",
    "        for pattern in patterns:\n",
    "            if pattern in predictor:\n",
    "                predictors.append(predictor)\n",
    "                break\n",
    "    return predictors\n",
    "\n",
    "\n",
    "def build_feature_sets(df):\n",
    "    readme_xls = utils.download_dataset(DATASET_README)\n",
    "    readme = pd.read_excel(readme_xls, sheet_name=\"Recipe_FINAL\")\n",
    "    \n",
    "    feature_sets = {}\n",
    "    \n",
    "    recipes = readme.iteritems()\n",
    "    next(recipes)   # first column is bogus\n",
    "    for recipe_num, recipe in recipes:\n",
    "        if recipe_num < 65:\n",
    "            continue\n",
    "        recipe_desc = recipe[3]\n",
    "        recipe_filter_1 = [filter for filter in (recipe[6], recipe[8]) if isinstance(filter, str)]\n",
    "        recipe_filter_2 = [filter for filter in recipe[10:] if isinstance(filter, str)]\n",
    "        recipe_name = f\"Recipe {recipe_num}: {recipe_desc}\"\n",
    "        feature_sets[recipe_name] = filter_predictors(filter_predictors(df.columns, recipe_filter_1), recipe_filter_2)\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "feature_sets = build_feature_sets(df)\n",
    "\n",
    "for feature_set_name, predictors in feature_sets.items():\n",
    "    sensors = set(map(predictor_sensor_number, predictors))\n",
    "    print(f\"{feature_set_name}\\n\\tPredictors: {len(predictors)}, Sensors: {len(sensors)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate boosted tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(target_name, feature_names):\n",
    "    X, y, groups = df[feature_names], df[target_name], df[\"M_Sub\"]\n",
    "    \n",
    "    model = pipeline.Pipeline([\n",
    "        ('scaler', preprocessing.StandardScaler()),\n",
    "        ('gboost', ensemble.HistGradientBoostingRegressor())\n",
    "    ])\n",
    "    \n",
    "    logo = model_selection.LeaveOneGroupOut()\n",
    "\n",
    "    prediction = model_selection.cross_val_predict(\n",
    "        model, X, y, cv=logo, groups=groups, n_jobs=-1)\n",
    "\n",
    "    r2_score = {}\n",
    "    for idx_train, idx_test in logo.split(df, groups=groups):\n",
    "        subject = df.iloc[idx_test[0]][\"M_Sub\"]\n",
    "        r2_score[subject] = metrics.r2_score(y.iloc[idx_test], prediction[idx_test])\n",
    "        \n",
    "    r2_score = pd.Series(r2_score)\n",
    "    prediction = pd.Series(prediction, index=y.index)\n",
    "    \n",
    "    # Feature importances on the full training set\n",
    "    model.fit(X, y)\n",
    "    perm_imp = inspection.permutation_importance(model, X, y, n_repeats=5, n_jobs=-1)\n",
    "    importance = pd.Series(perm_imp.importances_mean, index=X.columns)\n",
    "    importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    return r2_score, importance, prediction, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments, save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "target_name = \"TF_Pelvis_Moment_X_BWBH\"\n",
    "\n",
    "r2_mean_scores = {}\n",
    "\n",
    "for feature_set_name, feature_names in feature_sets.items():\n",
    "    r2_score, importance, prediction, target = evaluate(target_name, feature_names)\n",
    "    r2_mean_scores[feature_set_name] = r2_score.mean()\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"---\\n\"\n",
    "            f\"**Target**: {target_name}  \\n\"\n",
    "            f\"**Features**: {feature_set_name}  \\n\"\n",
    "            f\"**$R^2$ = {r2_mean_scores[feature_set_name]:.3f}**\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with pd.ExcelWriter(f\"{RESULTS_DIR}/R2_scores.xlsx\") as writer:\n",
    "        df_results = pd.DataFrame({f\"R2 - {target_name}\": r2_mean_scores})\n",
    "        df_results.to_excel(writer, sheet_name=\"R2 Scores\")\n",
    "\n",
    "\n",
    "    short_name = feature_set_name.split(\":\")[0].replace(\" \", \"_\")\n",
    "    with pd.ExcelWriter(f\"{RESULTS_DIR}/{short_name}_results.xlsx\") as writer:\n",
    "\n",
    "        df_results = pd.DataFrame({f\"R2 - {target_name}\": r2_score})\n",
    "        df_results.to_excel(writer, index_label=\"Test Subject\", sheet_name=\"R2 Scores\")\n",
    "\n",
    "\n",
    "        df_results = pd.DataFrame(\n",
    "            {\n",
    "                \"Short name\": map(predictor_short_name, importance.index),\n",
    "                f\"Importance - {target_name}\": importance,\n",
    "            }\n",
    "        )\n",
    "        df_results.to_excel(writer, sheet_name=\"Importance\")\n",
    "        \n",
    "    df_results = pd.DataFrame(\n",
    "        {\n",
    "            f\"Predictions - {target_name}\": prediction,\n",
    "            f\"Target - {target_name}\": target\n",
    "        }\n",
    "    )\n",
    "    df_results.to_csv(f\"{RESULTS_DIR}/{short_name}_predictions.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
